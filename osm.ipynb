{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 25861 properties\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load the original dataset\n",
    "df = pd.read_csv('cleaned_datasets/neighbourhood_warszawa_updated.csv')\n",
    "print(f\"Loaded dataset with {len(df)} properties\")\n",
    "\n",
    "# Define POI types and their max distances\n",
    "poi_types = {\n",
    "    'tramStation': 0.75,\n",
    "    'hospital': 1.0,\n",
    "    'trainStation': 2.0,\n",
    "    'outlinePark': 0.5,\n",
    "    'outlineRiver': 1.0,\n",
    "    'outlineAirport': 2.0,\n",
    "    'outlineExpressWay': 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate distance using Euclidean approximation\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    # For Warsaw latitude (~52 degrees), 1 degree of latitude ≈ 111.1 km\n",
    "    # 1 degree of longitude ≈ 67.4 km\n",
    "    lat_km = 111.1\n",
    "    lon_km = 67.4\n",
    "    \n",
    "    # Calculate distance using Euclidean approximation\n",
    "    dx = (lon2 - lon1) * lon_km\n",
    "    dy = (lat2 - lat1) * lat_km\n",
    "    \n",
    "    return math.sqrt(dx*dx + dy*dy)\n",
    "\n",
    "# Helper function for single property distance calculation\n",
    "def _calculate_single_min_distance(prop_lat, prop_lon, poi_lats, poi_lons, max_distance):\n",
    "    import numpy as np\n",
    "    \n",
    "    # Calculate squared distances first (avoids unnecessary sqrt operations)\n",
    "    lat_km = 111.1\n",
    "    lon_km = 67.4\n",
    "    \n",
    "    dx = (poi_lons - prop_lon) * lon_km\n",
    "    dy = (poi_lats - prop_lat) * lat_km\n",
    "    \n",
    "    # Calculate squared distances\n",
    "    squared_distances = dx*dx + dy*dy\n",
    "    \n",
    "    # Find the minimum distance\n",
    "    min_squared_dist = np.min(squared_distances)\n",
    "    \n",
    "    # Only calculate sqrt for the minimum\n",
    "    min_dist = np.sqrt(min_squared_dist)\n",
    "    \n",
    "    # Return None if distance is greater than max_distance\n",
    "    return min_dist if min_dist <= max_distance else None\n",
    "\n",
    "# Function to calculate min distance to nearest POI - optimized with vectorization\n",
    "def calculate_min_distance(properties_df, poi_locations, max_distance):\n",
    "    if not poi_locations:\n",
    "        return [None] * len(properties_df)\n",
    "        \n",
    "    # Convert POI locations to numpy arrays for faster computation\n",
    "    import numpy as np\n",
    "    poi_lats = np.array([loc[0] for loc in poi_locations])\n",
    "    poi_lons = np.array([loc[1] for loc in poi_locations])\n",
    "    \n",
    "    # For larger datasets, use vectorized operations\n",
    "    results = []\n",
    "    chunk_size = 1000  # Process in chunks to avoid memory issues\n",
    "    \n",
    "    for i in range(0, len(properties_df), chunk_size):\n",
    "        chunk = properties_df.iloc[i:i+chunk_size]\n",
    "        prop_lats = chunk['latitude'].values\n",
    "        prop_lons = chunk['longitude'].values\n",
    "        \n",
    "        # Calculate distances for all properties to all POIs\n",
    "        chunk_results = []\n",
    "        for j in range(len(chunk)):\n",
    "            min_dist = _calculate_single_min_distance(\n",
    "                prop_lats[j], prop_lons[j], poi_lats, poi_lons, max_distance\n",
    "            )\n",
    "            chunk_results.append(min_dist)\n",
    "        \n",
    "        results.extend(chunk_results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_osm_data(poi_type, city=\"Warszawa\"):\n",
    "    import os\n",
    "    import pickle\n",
    "    import random\n",
    "    \n",
    "    # Create cache directory if it doesn't exist\n",
    "    cache_dir = \"osm_cache\"\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "    \n",
    "    # Check if cached data exists\n",
    "    cache_file = os.path.join(cache_dir, f\"{poi_type}_{city}.pkl\")\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    overpass_url = \"https://overpass-api.de/api/interpreter\"\n",
    "    \n",
    "    # Different queries based on POI type\n",
    "    if poi_type == \"tramStation\":\n",
    "        overpass_query = \"\"\"\n",
    "        [out:json];\n",
    "        (\n",
    "          node[\"public_transport\"=\"stop_position\"][\"tram\"=\"yes\"](52.1, 20.8, 52.4, 21.3);\n",
    "          node[\"railway\"=\"tram_stop\"](52.1, 20.8, 52.4, 21.3);\n",
    "        );\n",
    "        out body;\n",
    "        \"\"\"\n",
    "    elif poi_type == \"hospital\":\n",
    "        overpass_query = \"\"\"\n",
    "        [out:json];\n",
    "        (\n",
    "          node[\"amenity\"=\"hospital\"](52.1, 20.8, 52.4, 21.3);\n",
    "          way[\"amenity\"=\"hospital\"](52.1, 20.8, 52.4, 21.3);\n",
    "          node[\"amenity\"=\"clinic\"](52.1, 20.8, 52.4, 21.3);\n",
    "        );\n",
    "        out center;\n",
    "        \"\"\"\n",
    "    elif poi_type == \"trainStation\":\n",
    "        overpass_query = \"\"\"\n",
    "        [out:json];\n",
    "        (\n",
    "          node[\"railway\"=\"station\"](52.1, 20.8, 52.4, 21.3);\n",
    "          way[\"railway\"=\"station\"](52.1, 20.8, 52.4, 21.3);\n",
    "          node[\"railway\"=\"halt\"](52.1, 20.8, 52.4, 21.3);\n",
    "        );\n",
    "        out center;\n",
    "        \"\"\"\n",
    "    elif poi_type == \"outlinePark\":\n",
    "        overpass_query = \"\"\"\n",
    "        [out:json];\n",
    "        (\n",
    "          way[\"leisure\"=\"park\"](52.1, 20.8, 52.4, 21.3);\n",
    "          way[\"landuse\"=\"forest\"](52.1, 20.8, 52.4, 21.3);\n",
    "        );\n",
    "        out geom;\n",
    "        \"\"\"\n",
    "    elif poi_type == \"outlineRiver\":\n",
    "        overpass_query = \"\"\"\n",
    "        [out:json];\n",
    "        (\n",
    "          way[\"waterway\"=\"river\"](52.1, 20.8, 52.4, 21.3);\n",
    "          way[\"natural\"=\"water\"](52.1, 20.8, 52.4, 21.3);\n",
    "        );\n",
    "        out geom;\n",
    "        \"\"\"\n",
    "    elif poi_type == \"outlineAirport\":\n",
    "        overpass_query = \"\"\"\n",
    "        [out:json];\n",
    "        (\n",
    "          way[\"aeroway\"=\"aerodrome\"](52.1, 20.8, 52.4, 21.3);\n",
    "          way[\"aeroway\"=\"runway\"](52.1, 20.8, 52.4, 21.3);\n",
    "        );\n",
    "        out geom;\n",
    "        \"\"\"\n",
    "    elif poi_type == \"outlineExpressWay\":\n",
    "        overpass_query = \"\"\"\n",
    "        [out:json];\n",
    "        (\n",
    "          way[\"highway\"=\"motorway\"](52.1, 20.8, 52.4, 21.3);\n",
    "          way[\"highway\"=\"trunk\"](52.1, 20.8, 52.4, 21.3);\n",
    "          way[\"highway\"=\"primary\"](52.1, 20.8, 52.4, 21.3);\n",
    "        );\n",
    "        out geom;\n",
    "        \"\"\"\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    # Add proper headers\n",
    "    headers = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'User-Agent': 'Python/Requests'\n",
    "    }\n",
    "    \n",
    "    # Try multiple times with error handling\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(overpass_url, data=overpass_query, headers=headers, timeout=30)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"  Warning: Status code {response.status_code} for {poi_type}, attempt {attempt+1}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(5)  # Wait before retry\n",
    "                    continue\n",
    "            \n",
    "            data = response.json()\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"  Error on attempt {attempt+1} for {poi_type}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(5)  # Wait before retry\n",
    "            else:\n",
    "                print(f\"  Failed to fetch data for {poi_type} after {max_retries} attempts\")\n",
    "                return []\n",
    "    \n",
    "    locations = []\n",
    "    \n",
    "    # For non-outline POIs, process normally with centers\n",
    "    if not poi_type.startswith(\"outline\"):\n",
    "        for element in data.get('elements', []):\n",
    "            if element.get('type') == 'node':\n",
    "                lat = element.get('lat')\n",
    "                lon = element.get('lon')\n",
    "                if lat and lon:\n",
    "                    locations.append((lat, lon))\n",
    "            else:  # way or relation with center point\n",
    "                center = element.get('center', {})\n",
    "                if center:\n",
    "                    lat = center.get('lat')\n",
    "                    lon = center.get('lon')\n",
    "                    if lat and lon:\n",
    "                        locations.append((lat, lon))\n",
    "    # For outline features, extract all points forming the geometry\n",
    "    else:\n",
    "        for element in data.get('elements', []):\n",
    "            if 'geometry' in element:\n",
    "                for geo_point in element['geometry']:\n",
    "                    lat = geo_point.get('lat')\n",
    "                    lon = geo_point.get('lon')\n",
    "                    if lat and lon:\n",
    "                        locations.append((lat, lon))\n",
    "    \n",
    "    # Log the number of points found\n",
    "    print(f\"  Retrieved {len(locations)} coordinate points for {poi_type}\")\n",
    "    \n",
    "    # Subsample large point sets to improve performance\n",
    "    if poi_type.startswith(\"outline\") and len(locations) > 1000:\n",
    "        locations = random.sample(locations, 1000)\n",
    "        print(f\"  Sampled down to {len(locations)} points for performance\")\n",
    "    \n",
    "    # Cache the results\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(locations, f)\n",
    "            \n",
    "    return locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_poi_type(poi_type_info):\n",
    "    poi_type, max_distance = poi_type_info\n",
    "    start_time = time.time()\n",
    "    print(f\"Processing {poi_type}...\")\n",
    "    \n",
    "    # Fetch POI locations (cached if available)\n",
    "    poi_locations = fetch_osm_data(poi_type)\n",
    "    print(f\"  Found {len(poi_locations)} {poi_type} locations\")\n",
    "    \n",
    "    # Calculate minimum distances for each property\n",
    "    column_name = f\"minDistTo{poi_type.replace('outline', '')}\"\n",
    "    \n",
    "    # Use the optimized function\n",
    "    distances = calculate_min_distance(df, poi_locations, max_distance)\n",
    "    \n",
    "    # Count properties with this POI within range\n",
    "    count_within_range = sum(1 for d in distances if d is not None)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"  {count_within_range} properties have a {poi_type} within {max_distance}km (took {elapsed_time:.2f}s)\")\n",
    "    \n",
    "    return poi_type, column_name, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel processing of 7 POI types...\n",
      "Processing tramStation...\n",
      "Processing hospital...\n",
      "Processing trainStation...\n",
      "Processing outlinePark...\n",
      "  Found 657 tramStation locations\n",
      "  Found 236 hospital locations\n",
      "  Found 1000 outlinePark locations\n",
      "  Found 117 trainStation locations\n",
      "  20302 properties have a hospital within 1.0km (took 0.45s)  21061 properties have a trainStation within 2.0km (took 0.46s)\n",
      "Processing outlineRiver...\n",
      "\n",
      "Processing outlineAirport...\n",
      "  Found 802 outlineAirport locations\n",
      "  Found 1000 outlineRiver locations\n",
      "  16524 properties have a tramStation within 0.75km (took 4.61s)\n",
      "Processing outlineExpressWay...\n",
      "  Found 1000 outlineExpressWay locations\n",
      "  3343 properties have a outlineAirport within 2.0km (took 4.86s)\n",
      "  11648 properties have a outlinePark within 0.5km (took 5.54s)\n",
      "  18044 properties have a outlineRiver within 1.0km (took 5.10s)\n",
      "  6993 properties have a outlineExpressWay within 0.5km (took 1.17s)\n",
      "All POI types processed in 5.79 seconds\n",
      "Dataset updated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Process POI types in parallel\n",
    "print(f\"Starting parallel processing of {len(poi_types)} POI types...\")\n",
    "start_total = time.time()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=min(len(poi_types), 4)) as executor:\n",
    "    # Submit all tasks\n",
    "    future_to_poi = {\n",
    "        executor.submit(process_poi_type, (poi_type, max_distance)): poi_type\n",
    "        for poi_type, max_distance in poi_types.items()\n",
    "    }\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in future_to_poi:\n",
    "        try:\n",
    "            poi_type, column_name, distances = future.result()\n",
    "            # Update DataFrame with results\n",
    "            df[column_name] = distances\n",
    "            # Create categorical column\n",
    "            df[poi_type] = df[column_name].apply(\n",
    "                lambda x: x if x is not None else None\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {future_to_poi[future]}: {e}\")\n",
    "\n",
    "elapsed_total = time.time() - start_total\n",
    "print(f\"All POI types processed in {elapsed_total:.2f} seconds\")\n",
    "\n",
    "# Nullify existing columns based on distance thresholds\n",
    "df['cityCentre'] = df['mindistocenter'].apply(lambda x: x if pd.notnull(x) else None)\n",
    "df['metroStation'] = df['mindistometro'].apply(lambda x: x if pd.notnull(x) and x <= 2.0 else None)\n",
    "df['uni'] = df['mindistouni'].apply(lambda x: x if pd.notnull(x) and x <= 2.0 else None)\n",
    "df['mall'] = df['mindistotradecenter'].apply(lambda x: x if pd.notnull(x) and x <= 1.0 else None)\n",
    "\n",
    "# Save the updated dataset\n",
    "df.to_csv('cleaned_datasets/neighbourhood_warszawa_updated_with_pois.csv', index=False)\n",
    "\n",
    "print(\"Dataset updated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['pricePerSqm', 'latitude', 'longitude', 'minDistToMetro', 'minDistToUni', 'minDistToMall', 'minDistToCenter', 'minDistToTramStation', 'tramStation', 'minDistToHospital', 'hospital', 'minDistToTrainStation', 'trainStation', 'minDistToPark', 'outlinePark', 'minDistToRiver', 'outlineRiver', 'minDistToAirport', 'outlineAirport', 'minDistToExpressWay', 'outlineExpressWay', 'cityCentre', 'metroStation', 'uni', 'mall']\n",
      "Final dataset saved with distance columns!\n",
      "Columns in final dataset: longitude, latitude, pricePerSqm, minDistToMetro, minDistToUni, minDistToMall, minDistToCenter, minDistToTramStation, minDistToHospital, minDistToTrainStation, minDistToPark, minDistToRiver, minDistToAirport, minDistToExpressWay\n"
     ]
    }
   ],
   "source": [
    "# Rename distance columns to camelCase with proper capitalization\n",
    "column_renames = {\n",
    "    # Original columns\n",
    "    'mindistometro': 'minDistToMetro',\n",
    "    'mindistouni': 'minDistToUni',\n",
    "    'mindistotradecenter': 'minDistToMall',\n",
    "    'mindistocenter': 'minDistToCenter',\n",
    "    \n",
    "    # New columns - fix capitalization\n",
    "    'minDistTotramStation': 'minDistToTramStation',\n",
    "    'minDistTohospital': 'minDistToHospital',\n",
    "    'minDistTotrainStation': 'minDistToTrainStation',\n",
    "    'minDistToPark': 'minDistToPark',\n",
    "    'minDistToRiver': 'minDistToRiver',\n",
    "    'minDistToAirport': 'minDistToAirport',\n",
    "    'minDistToExpressWay': 'minDistToExpressWay'\n",
    "}\n",
    "\n",
    "# Apply the renames to existing columns only\n",
    "for old_name, new_name in column_renames.items():\n",
    "    if old_name in df.columns:\n",
    "        df = df.rename(columns={old_name: new_name})\n",
    "\n",
    "# Print all columns to debug\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "\n",
    "# Create a list of columns to keep\n",
    "columns_to_keep = []\n",
    "\n",
    "# Add longitude and latitude to columns_to_keep if they exist\n",
    "if 'longitude' in df.columns:\n",
    "    columns_to_keep.append('longitude')\n",
    "else:\n",
    "    print(\"Warning: 'longitude' column not found in the dataset\")\n",
    "    \n",
    "if 'latitude' in df.columns:\n",
    "    columns_to_keep.append('latitude')\n",
    "else:\n",
    "    print(\"Warning: 'latitude' column not found in the dataset\")\n",
    "\n",
    "# Add pricePerSqm if it exists\n",
    "if 'pricePerSqm' in df.columns:\n",
    "    columns_to_keep.append('pricePerSqm')\n",
    "else:\n",
    "    print(\"Warning: 'pricePerSqm' column not found in the dataset\")\n",
    "\n",
    "# Add all distance columns that exist\n",
    "columns_to_keep.extend([col for col in df.columns if col.startswith('minDistTo')])\n",
    "\n",
    "# Verify all expected distance columns exist\n",
    "expected_columns = [\n",
    "    'minDistToMetro', 'minDistToUni', 'minDistToMall', 'minDistToCenter',\n",
    "    'minDistToTramStation', 'minDistToHospital', 'minDistToTrainStation',\n",
    "    'minDistToPark', 'minDistToRiver', 'minDistToAirport', 'minDistToExpressWay'\n",
    "]\n",
    "\n",
    "# Check for any missing distance columns\n",
    "missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Warning: These expected columns are missing: {', '.join(missing_columns)}\")\n",
    "\n",
    "# Keep only the selected columns (that exist)\n",
    "if columns_to_keep:\n",
    "    df = df[columns_to_keep]\n",
    "    \n",
    "    # Save the final dataset\n",
    "    df.to_csv('cleaned_datasets/neighbourhood_warszawa_distances.csv', index=False)\n",
    "    \n",
    "    print(\"Final dataset saved with distance columns!\")\n",
    "    print(f\"Columns in final dataset: {', '.join(df.columns)}\")\n",
    "else:\n",
    "    print(\"Error: No columns to keep. Check column names in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: osmnx in ./.venv/lib/python3.12/site-packages (2.0.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: geopandas>=1.0 in ./.venv/lib/python3.12/site-packages (from osmnx) (1.0.1)\n",
      "Requirement already satisfied: networkx>=2.5 in ./.venv/lib/python3.12/site-packages (from osmnx) (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.22 in ./.venv/lib/python3.12/site-packages (from osmnx) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.4 in ./.venv/lib/python3.12/site-packages (from osmnx) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.27 in ./.venv/lib/python3.12/site-packages (from osmnx) (2.32.3)\n",
      "Requirement already satisfied: shapely>=2.0 in ./.venv/lib/python3.12/site-packages (from osmnx) (2.0.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in ./.venv/lib/python3.12/site-packages (from geopandas>=1.0->osmnx) (0.10.0)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in ./.venv/lib/python3.12/site-packages (from geopandas>=1.0->osmnx) (3.7.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas>=1.4->osmnx) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas>=1.4->osmnx) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.27->osmnx) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.27->osmnx) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.27->osmnx) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.27->osmnx) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install osmnx matplotlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
